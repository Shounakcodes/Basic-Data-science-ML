{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "19e6fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,SimpleRNN,Embedding,Flatten\n",
    "from keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec,KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0bba94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading imdb data set and merging train and test to get a net data set\n",
    "\n",
    "(X_train,y_train),(X_test,y_test) = imdb.load_data()\n",
    "X= np.concatenate((X_train,X_test),axis=0)\n",
    "Y= np.concatenate((y_train,y_test),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "90c4ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse indexing words and storinf them in decoded review list\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index, decoded_review={},[]\n",
    "\n",
    "for i,j in word_index.items():\n",
    "    reverse_word_index[j]=i\n",
    "\n",
    "for z in range(0, len(X)):\n",
    "    token = ' '.join([reverse_word_index.get(i - 3, '?') for i in X[z]])\n",
    "    decoded_review.append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a919f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lematizing the words in each sentence of decoded review and storing in final tokens\n",
    "\n",
    "final_tokens=[]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(0,int(len(decoded_review)/100)):\n",
    "    decoded_review[i]= re.sub('[^a-zA-Z]',' ',decoded_review[i])\n",
    "    decoded_review[i]= decoded_review[i].lower().split()\n",
    "    token= [lemmatizer.lemmatize(words) for words in decoded_review[i] if words not in stopwords.words('English')]\n",
    "    final_tokens.append(token)\n",
    "\n",
    "Y= Y[0:int(len(decoded_review)/100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "58801df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training word2vec to get embeddin/gs for each word for every sentence\n",
    "\n",
    "final_tokens= [i for i in final_tokens if i != []]\n",
    "model= gensim.models.Word2Vec(final_tokens,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d948bef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 50, 100)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def w2vec(sentence, model):\n",
    "    w_arr=[]\n",
    "    for words in sentence:\n",
    "        if(words in model.wv):\n",
    "            w_arr.append(model.wv[words])\n",
    "    # w_arr= model.wv[sentence]\n",
    "\n",
    "    if(len(w_arr)==0):\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return w_arr\n",
    "    \n",
    "\n",
    "w2v_arr=[]\n",
    "for c in final_tokens:\n",
    "    vectors= w2vec(c, model)\n",
    "    w2v_arr.append(vectors)\n",
    "\n",
    "X_padded = pad_sequences(w2v_arr, dtype='float32', padding='post',maxlen=50)\n",
    "\n",
    "np.shape(X_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "10e738cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "X_train,X_test=  X_padded[0:int(0.8*len(X_padded))], X_padded[int(0.8*len(X_padded)):]\n",
    "Y_train,Y_test= Y[0:int(0.8*len(X_padded))],Y[int(0.8*len(X_padded)):]\n",
    "\n",
    "shape1, shape2=np.shape(X_train)[1], np.shape(X_train)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c8fab3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training rnn\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout,LSTM\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model_RNN = Sequential()\n",
    "\n",
    "# Add a SimpleRNN layer\n",
    "model_RNN.add(Input(shape=(shape1,shape2)))\n",
    "model_RNN.add(LSTM(64))\n",
    "\n",
    "# Optional: Add dropout for regularization\n",
    "# model_RNN.add(Dropout(0.5))\n",
    "\n",
    "# Output layer (for binary classification)\n",
    "model_RNN.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "15ecfd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.4560 - loss: 0.7007 - val_accuracy: 0.4500 - val_loss: 0.7005\n",
      "Epoch 2/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5254 - loss: 0.6895 - val_accuracy: 0.4750 - val_loss: 0.7002\n",
      "Epoch 3/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5083 - loss: 0.6887 - val_accuracy: 0.5375 - val_loss: 0.6936\n",
      "Epoch 4/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.5498 - loss: 0.6867 - val_accuracy: 0.4750 - val_loss: 0.7035\n",
      "Epoch 5/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5463 - loss: 0.6854 - val_accuracy: 0.4875 - val_loss: 0.6950\n",
      "Epoch 6/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5734 - loss: 0.6873 - val_accuracy: 0.4750 - val_loss: 0.7099\n",
      "Epoch 7/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5635 - loss: 0.6816 - val_accuracy: 0.4875 - val_loss: 0.7000\n",
      "Epoch 8/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5740 - loss: 0.6817 - val_accuracy: 0.4750 - val_loss: 0.7064\n",
      "Epoch 9/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5433 - loss: 0.6825 - val_accuracy: 0.4875 - val_loss: 0.6975\n",
      "Epoch 10/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.5902 - loss: 0.6730 - val_accuracy: 0.4875 - val_loss: 0.7144\n",
      "Epoch 11/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6026 - loss: 0.6757 - val_accuracy: 0.4875 - val_loss: 0.7094\n",
      "Epoch 12/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6199 - loss: 0.6613 - val_accuracy: 0.4375 - val_loss: 0.7158\n",
      "Epoch 13/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5683 - loss: 0.6616 - val_accuracy: 0.4750 - val_loss: 0.7517\n",
      "Epoch 14/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6271 - loss: 0.6466 - val_accuracy: 0.5000 - val_loss: 0.7245\n",
      "Epoch 15/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6217 - loss: 0.6593 - val_accuracy: 0.3875 - val_loss: 0.7554\n",
      "Epoch 16/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5915 - loss: 0.6521 - val_accuracy: 0.4750 - val_loss: 0.7644\n",
      "Epoch 17/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5813 - loss: 0.6597 - val_accuracy: 0.4250 - val_loss: 0.7465\n",
      "Epoch 18/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5910 - loss: 0.6556 - val_accuracy: 0.4625 - val_loss: 0.7489\n",
      "Epoch 19/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6588 - loss: 0.6362 - val_accuracy: 0.4875 - val_loss: 0.7794\n",
      "Epoch 20/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6492 - loss: 0.6279 - val_accuracy: 0.4500 - val_loss: 0.7489\n",
      "Epoch 21/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6071 - loss: 0.6404 - val_accuracy: 0.5000 - val_loss: 0.7590\n",
      "Epoch 22/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6994 - loss: 0.6203 - val_accuracy: 0.4750 - val_loss: 0.7831\n",
      "Epoch 23/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6903 - loss: 0.6175 - val_accuracy: 0.4625 - val_loss: 0.7815\n",
      "Epoch 24/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6276 - loss: 0.6272 - val_accuracy: 0.4375 - val_loss: 0.7651\n",
      "Epoch 25/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6392 - loss: 0.6137 - val_accuracy: 0.3875 - val_loss: 0.7915\n",
      "Epoch 26/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6710 - loss: 0.6047 - val_accuracy: 0.4375 - val_loss: 0.8176\n",
      "Epoch 27/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6537 - loss: 0.5985 - val_accuracy: 0.4375 - val_loss: 0.7658\n",
      "Epoch 28/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6777 - loss: 0.5950 - val_accuracy: 0.4375 - val_loss: 0.8082\n",
      "Epoch 29/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6625 - loss: 0.5885 - val_accuracy: 0.4625 - val_loss: 0.7725\n",
      "Epoch 30/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6965 - loss: 0.5856 - val_accuracy: 0.4500 - val_loss: 0.8296\n",
      "Epoch 31/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6637 - loss: 0.5786 - val_accuracy: 0.4375 - val_loss: 0.7843\n",
      "Epoch 32/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6667 - loss: 0.5953 - val_accuracy: 0.4500 - val_loss: 0.8348\n",
      "Epoch 33/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6606 - loss: 0.5921 - val_accuracy: 0.5000 - val_loss: 0.7552\n",
      "Epoch 34/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6811 - loss: 0.5789 - val_accuracy: 0.4625 - val_loss: 0.8422\n",
      "Epoch 35/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6660 - loss: 0.5868 - val_accuracy: 0.4500 - val_loss: 0.8023\n",
      "Epoch 36/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7010 - loss: 0.5856 - val_accuracy: 0.4625 - val_loss: 0.7862\n",
      "Epoch 37/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6823 - loss: 0.5570 - val_accuracy: 0.4500 - val_loss: 0.8135\n",
      "Epoch 38/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6557 - loss: 0.5852 - val_accuracy: 0.5250 - val_loss: 0.7778\n",
      "Epoch 39/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6665 - loss: 0.5607 - val_accuracy: 0.4500 - val_loss: 0.8777\n",
      "Epoch 40/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7123 - loss: 0.5446 - val_accuracy: 0.4500 - val_loss: 0.8151\n",
      "Epoch 41/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7121 - loss: 0.5598 - val_accuracy: 0.4375 - val_loss: 0.8452\n",
      "Epoch 42/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6981 - loss: 0.5496 - val_accuracy: 0.4500 - val_loss: 0.8040\n",
      "Epoch 43/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6825 - loss: 0.5774 - val_accuracy: 0.4500 - val_loss: 0.8423\n",
      "Epoch 44/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6635 - loss: 0.5798 - val_accuracy: 0.4625 - val_loss: 0.7948\n",
      "Epoch 45/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7449 - loss: 0.5458 - val_accuracy: 0.4625 - val_loss: 0.8211\n",
      "Epoch 46/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7441 - loss: 0.5091 - val_accuracy: 0.4375 - val_loss: 0.8370\n",
      "Epoch 47/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7305 - loss: 0.5289 - val_accuracy: 0.4375 - val_loss: 0.8638\n",
      "Epoch 48/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6895 - loss: 0.5602 - val_accuracy: 0.4500 - val_loss: 0.8489\n",
      "Epoch 49/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7481 - loss: 0.5215 - val_accuracy: 0.5000 - val_loss: 0.8182\n",
      "Epoch 50/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7113 - loss: 0.5248 - val_accuracy: 0.4250 - val_loss: 0.8746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23580ab7590>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_RNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_RNN.fit(X_train, Y_train, epochs=50, batch_size=32, validation_split=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
